Attention is a widely popular topic studied in many fields such as neuroscience, psychology, and machine learning. A better understanding and conceptualization of attention in both humans and machines has led to significant progress across fields. At the same time, attention is far from a clear or unified concept, with many definitions within and across multiple fields.

Cognitive scientists study how the brain flexibly controls its limited computational resources to accomplish its objectives. Inspired by cognitive attention, machine learning researchers introduce attention as an inductive bias in their models to improve performance or interpretability. Human-computer interaction designers monitor people’s attention during interactions to implicitly detect aspects of their mental states.

While the aforementioned research areas all consider attention, each formalizes and operationalizes it in different ways. Bridging this gap will facilitate:

- (Cogsci for AI) More principled forms of attention in AI agents towards more human-like abilities such as robust generalization, quicker learning and faster planning.
- (AI for cogsci) Developing better computational models for modeling human behaviors that involve attention.
- (HCI) Modeling attention during interactions from implicit signals for fluent and efficient coordination
- (HCI/ML) Artificial models of algorithmic attention to enable intuitive interpretations of deep models?


## Topics of Interest



The All Things Attention workshop aims to foster connections across disparate academic communities that conceptualize "attention" such as Neuroscience, Psychology, Machine Learning, and Human Computer Interaction. Workshop topics of interest include:

### Relationships between biological and artificial attention
  - What are the connections between forms of attention in the human brain and deep neural network architectures? 
  - Can the anatomy of human attention models inspire designs of architectures for artificial systems? 
  - Given the same task and learning objective, how do learned attention mechanisms in machines differ from those in humans? 

### Attention for reinforcement learning and decision making
  - How have reinforcement learning agents leveraged attention in decision making?
  - Do decision-making agents today have implicit or explicit formalisms of attention?
  - How can AI agents build notions of attention without explicitly baked in notions of attention?
  - Can attention significantly enable AI agents to scale e.g. through gains in sample efficiency, and generalization?
  - How should learning systems reason about computational attention (which parts of sensed inputs to focus computation on)?

### Attention mechanisms for continual / lifelong learning
  - How can continual learning agents use attention to maintain already-learned knowledge? 
  - How can attention control the amount of interference between different inputs? 
  - How does the executive control of attention evolve with learning in humans? 
  - How does understanding the development of human attentional systems in infancy and childhood explain how attention can be learned in artificial systems?

### Attention for interpretation and explanation
  - How can attention models aid visualization?
  - How is attention used for interpretability in AI? 
  - What are the major bottlenecks and common pitfalls in applying attention methods for explaining the decisions of AI agents?

### Attention in human-computer interaction
  - How do we detect aspects of human attention during interactions, from sensing to processing to representations?
  - What systems benefit from human attention modeling, and how do they use these models?
  - How can systems influence a user’s attention, and what systems benefit from this capability?
  - How can a system communicate or simulate its own attention (humanlike or algorithmic) in an interaction, and to what benefit?

### Attention mechanisms in Deep Neural Network (DNN) architectures
  - How does attention in DNNs such as transformers relate to existing formalisms of attention in cogsci/psychology? 
  - How does self-attention in transformers contribute to its vast success in recent models such as GPT2, GPT3, DALLE? 
  - How can an understanding of attention from other fields inspire future DNN research?


{% comment %}
This workshop will bring together area experts and student researchers to discuss the advances that have been made in the field of attention-based learning and the major challenges for future research efforts. 

The topics to be discussed include, but are not limited to:

- Exploring the connections between different forms of attention in ML
  - Relationships between biological and artificial attention
  - Attention learning in infants
  - Evolving executive control of attention in humans
- Attention for reinforcement learning and decision making
  - Architectures of attention mechanisms in Deep Neural Networks (DNN)
  - Task dependence of attention
  - Understanding the benefits of self-attention (transformers)
  - Benefits and formulation of attention mechanisms for continual / lifelong learning
- Attention as a tool for interpretation and explanation
  - Attention as an explanation/model introspection mehcanism
  - Image saliency as a proxy for attention
- The role of attention in human-computer interaction and human-robot interaction
  - Determining people’s attention
  - Responding to one's own/another’s attention
  - Communicating robot attention
  - Leveraging understanding of attention to enhance communication
{% endcomment %}

