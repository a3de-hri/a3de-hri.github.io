---
layout: layout_2022
urltitle:  "A Focus on Attention"
title: "A Focus on Attention"
categories: neurips, attention, workshop, computer vision, robotics, machine learning, gaze, interaction, neuroscience, cognitive psychology
permalink: /2022/
favicon: /2022/img/icon.jpg
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <!-- <img class="img-fluid" src="{{ "img/banner.png" | prepend:site.baseurl }}"> -->
    <!-- <small style="float:right;margin-top:1mm;margin-right:12mm;">Image credit to <a href="https://pixabay.com/zh/users/nika_akin-13521770/" target="_blank">Nika_Akin</a></small> -->
    <p><center>
    <h1>
    All things Attention: Bridging different perspectives on attention
    </h1>
    </center></p>
    <br><br>
    <p><center>
      In conjunction with NeurIPS 22: December 3, 2022 (in-person)
      <!-- <br><br>
      <b>7 papers</b> have been accepted in our GAZE2022 workshop.
      <br>
      Congratulations to all authors!
      <br><br>
      This year, June 19 and 20 marks Juneteenth, a US holiday commemorating the end of slavery in the US, and a holiday of special significance in the US South. We encourage attendees to learn more about Juneteenth and its historical context, and to join the city of New Orleans in celebrating the Juneteenth holiday. You can find out more information about Juneteenth here: <a href="https://cvpr2022.thecvf.com/recognizing-juneteenth" target="_blank">https://cvpr2022.thecvf.com/recognizing-juneteenth</a> -->
      <!-- Coming soon -->
      <br>
      <!-- 3pm - 8:20pm UTC -->
      <br><br>
      <!-- <table id="top-table">
        <style>
	  #top-table td {
	    padding: 1px 5px;
	  }
	  #top-table td:nth-child(1),
	  #top-table td:nth-child(3) {
	    text-align: right;
	    padding-right: 0px;
	  }
	</style>
        <tr><td>8am   </td><td>- 1:20pm     </td><td> PDT</td><td>(UTC-7)  </td></tr>
        <tr><td>11am  </td><td>- 4:20pm     </td><td> EDT</td><td>(UTC-4)  </td></tr>
        <tr><td>4pm   </td><td>- 9:20pm     </td><td> BST</td><td>(UTC+1)  </td></tr>
        <tr><td>5pm   </td><td>- 10:20pm    </td><td>CEST</td><td>(UTC+2)  </td></tr>
        <tr><td>8:30pm</td><td>- 1:50am (+1)</td><td> IST</td><td>(UTC+5.5)</td></tr>
        <tr><td>11pm  </td><td>- 4:20am (+1)</td><td> CST</td><td>(UTC+8)  </td></tr>
        <tr><td>12am  </td><td>- 5:20am (+1)</td><td> KST</td><td>(UTC+9)  </td></tr>
      </table> -->
      <!-- <br>
      Youtube recording: <a href="https://youtu.be/WQ8azMW_dn8" target="_blank">https://youtu.be/WQ8azMW_dn8</a> -->
    </center></p>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="intro"></a>
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>

Attention is a widely popular topic studied in many fields such as neuroscience, psychology, and machine learning. A better understanding, conceptualization, and operationalization of attention in both humans and machines has led to significant progress across fields. At the same time, attention is far from a clear or unified concept, with many definitions within and across multiple fields. 

Cognitive scientists study how the brain flexibly controls its limited computational resources to accomplish its objectives. Inspired by cognitive attention, machine learning researchers introduce attention as an inductive bias in their models to improve performance or interpretability. Human-computer interaction designers monitor peopleâ€™s attention during interactions to implicitly detect aspects of their mental states. 

The workshop "A Focus on Attention" at <a href="https://neurips.cc/" target="_blank">NeurIPS 2022</a> attempts to bridge the gap between the various concpetualizations of attention that exist across fields such as neuroscience, psychology, machine learning, and human-computer interaction. 


Specifically, the workshop topics include (but are not limited to):
    </p>
    <ul>
      <li>Reformulating eye detection, gaze estimation, and gaze prediction pipelines with deep networks.</li>
      <li>Applying geometric and anatomical constraints into the training of (sparse or dense) deep networks.</li>
      <li>Leveraging additional cues such as contexts from face region and head pose information.</li>
      <li>Developing adversarial methods to deal with conditions where current methods fail (illumination, appearance, etc.).</li>
      <li>Exploring attention mechanisms to predict the point of regard.</li>
      <li>Designing new accurate measures to account for rapid eye gaze movement.</li>
      <li>Novel methods for temporal gaze estimation and prediction including Bayesian methods.</li>
      <li>Integrating differentiable components into 3D gaze estimation frameworks.</li>
      <li>Robust estimation from different data modalities such as RGB, depth, head pose, and eye region landmarks.</li>
      <li>Generic gaze estimation method for handling extreme head poses and gaze directions.</li>
      <li>Temporal information usage for eye tracking to provide consistent gaze estimation on the screen.</li>
      <li>Personalization of gaze estimators with few-shot learning.</li>
      <li>Semi-/weak-/un-/self- supervised leraning methods, domain adaptation methods, and other novel methods towards improved representation learning from eye/face region images or gaze target region images.</li>
    </ul>
We will be hosting 2 invited speakers for the topic of gaze estimation. We will also be accepting the submission of full unpublished papers as done in previous versions of the workshop. These papers will be peer-reviewed via a double-blind process, and will be published in the official workshop proceedings and be presented at the workshop itself. More information will be provided as soon as possible.
  </div>
</div> <br>

<!-- CfP stuff -->
<div class="row">
  <div class="col-xs-12 panel-group"><a class="anchor" id="calls"></a>
    <h2>Call for Contributions</h2>
    <br>

    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-papers" style="cursor:pointer;">
        <h3 style="margin:0;">Full Workshop Papers</h3>
      </div>
      <div id="call-papers" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
          <p>
	    <span style="font-weight:500;">Submission:</span> We invite submission of papers (up to 8 pages, excluding references and appendix) in the NeurIPS 2022 format. The final submission including main paper, references and appendix should not exceed 12 pages. Supplementary Materials uploads are to only be used optionally for extra videos/code/data/figures and should be uploaded separately in the submission website. The review process is double-blind so the submission should be anonymized. Accepted work will be presented as posters during the workshop, and select contributions will be invited to give spotlight talks. Each accepted work entering the poster sessions will have an accompanying pre-recorded 5-minute video. Please note that at least one coauthor of each accepted paper will be expected to have a NeurIPS conference registration and participate in one of the poster sessions.  Submissions will be evaluated based on novelty, rigor, and relevance to the theme of the workshop. Both empirical and theoretical contributions are welcome. Submissions should not have previously appeared in a journal or conference (including accepted papers to NeurIPS 2022) and should not be submitted to another NeurIPS workshop. Submissions must adhere to the <a href="https://neurips.cc/public/CodeOfConduct" target="_blank">NeurIPS Code of Conduct</a>.
      Submissions will be accepted through <a href="" target="_blank">this OpenReview link (TBA)</a>
          </p>
      <!-- TODO links for submission -->
      <p>
      The focus of the work should relate to the list of the topics specified below. The review process will be double-blind and accepted submissions will be presented as virtual talks or posters. There will be no proceedings for this workshop, however, authors can opt to have their abstracts/papers posted on the workshop website.
      </p>
      We encourage submissions on the following topics from the focus of bridging different perspectives on attention:
    <ul>
    <li>  Relationships between biological and artificial attention </li>
    <li>  Attention for reinforcement learning and decision making </li>
    <li>  Benefits and formulation of attention mechanisms for continual / lifelong learning </li>
    <li>  Attention as a tool for interpretation and explanation </li>
    <li>  The role of attention in human-computer interaction and human-robot interaction </li>
    <li>  Attention mechanisms in Deep Neural Network (DNN) architectures </li>
    </ul>
	  <p>
	    <span style="font-weight:500;">Note:</span> Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as <code>previous_reviews.pdf</code>) and a letter of changes (named as <code>letter_of_changes.pdf</code>) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
	    <!--Due to potential clashes with the main conference reviewing schedule, we will accept simultaneous submissions to the ICCV main conference and GAZE Workshop. Simultaneous submissions are otherwise disallowed.-->
          </p>
        </div>
      </div>
    </div>
    <br>
    <!--
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-ea" style="cursor:pointer;">
        <h3 style="margin:0;">Extended Abstracts</h3>
      </div>
      <div id="call-ea" class="panel-collapse collapse in" data-parent="#call">
        <div class="panel-body">
          <p>
            In addition to regular papers, we also invite extended abstracts of ongoing or published work (<i>e.g.</i> related papers on ECCV main track). The extended abstracts will not be published or made available to the public (we will only list titles on our website) but will rather be presented during our poster session. We see this as an opportunity for authors to promote their work to an interested audience to gather valuable feedback.
          </p>
	  <p>
	    Further details on how this poster session will occur online, is to be decided. In general, we will be following the main ECCV conference guidelines and organization in organizing the presentation of all posters.
          </p>
          <p>Extended abstracts are limited to six pages and must be created using the <a href="https://eccv2020.eu/author-instructions/" target="_blank">official ECCV format</a>. The submission must be sent to <a href="mailto:openeyes.workshop@gmail.com">openeyes.workshop@gmail.com</a> by the deadline (July 17th).
          </p>
          <p>
            We will evaluate and notify authors of acceptance as soon as possible (evaluation on a rolling basis until the deadline) after receiving their extended abstract submission.
          </p>
        </div>
      </div>
    </div>
    <br>
    -->
    <!-- <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-challenge" style="cursor:pointer;">
        <h3 style="margin:0;">GAZE 2021 Challenges</h3>
      </div>
      <div id="call-challenge" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
    Coming soon
	  <p>
	    The GAZE 2021 Challenges are hosted on Codalab, and can be found at:
	  </p>
	  <ul>
	    <li>ETH-XGaze Challenge: <a href="https://competitions.codalab.org/competitions/28930">https://competitions.codalab.org/competitions/28930</a></li>
	    <li>EVE Challenge: <a href="https://competitions.codalab.org/competitions/28954">https://competitions.codalab.org/competitions/28954</a></li>
	  </ul>
	  <p>
            More information on the respective challenges can be found on their pages.
	  </p>
	  <br>
	  <p>
	    We are thankful to our sponsors for providing the following prizes:
	    <table style="width: 100%;">
	      <colgroup>
                <col span="1" style="width: 30%;">
		<col span="1" style="width: 55%;">
		<col span="1" style="width: 15%;">
	      </colgroup>
	      <tbody>
	        <tr>
	          <td><b>ETH-XGaze Challenge Winner</b></td>
	          <td>USD 500</td>
	          <td><small>courtesy of </small><img width="50" src="img/google.png"/></td>
	        </tr>
	        <tr>
	          <td><b>EVE Challenge Winner</b></td>
	          <td>Tobii Eye Tracker 5</td>
	          <td><small>courtesy of </small><img width="50" src="img/tobii.jpg"/></td>
	        </tr>
	      </tbody>
	    </table>
	  </p>
        </div>
      </div>
    </div> -->
  </div>
</div><br>

<!--  Imp dates -->
<div class="row">
  <div class="col-xs-12"><a class="anchor" id="dates"></a>
    <h2>Important Dates</h2>
    <br>
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>Sep 15, 2022 (11:59PM AoE)</td>
	      </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>October 12, 2022</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>November 25, 2022 (11:59PM AoE)</td>
        </tr>
        <!-- <tr>
          <td>ETH-XGaze &amp; EVE Challenges Closed</td>
          <td>May 28, 2021 (23:59 UTC)</td>
	  <td><span class="countdown" reference="28 May 2022 23:59:59 UTC"></span></td>
        </tr> -->
      </tbody>
    </table>
  </div>
</div><br>

<!-- Schedule -->
<div class="row">
  <div class="col-xs-12"><a class="anchor" id="schedule"></a>
     <h2>Workshop Schedule</h2>
     <br>
     <!-- <p>
       Attending:
       <ul>
         <li>Registered CVPR attendees can find the relevant Zoom and Gatherly links at <a target="_blank" href="https://www.eventscribe.net/2021/2021CVPR/login.asp">https://www.eventscribe.net/2021/2021CVPR/login.asp</a></li>
	 <li>Others are welcome to join our livestream at <a href="https://youtu.be/ScoHuri_3hs">https://youtu.be/ScoHuri_3hs</a></li>
       </ul>
     </p> -->
     <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
	  <th>Time in UTC</th>
	  <th>Start Time in UTC<span class="tz-offset"></span><b>*</b><br><span class="tz-subtext">(probably your time zone)</span></th>
          <th>Item</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>1:30pm - 1:35pm</td>
          <td class="to-local-time">20 Jun 2022 13:30:00 UTC</td>
          <td>Opening remark</td>
        </tr>
        <tr>
          <td>1:35pm - 2:15pm</td>
          <td class="to-local-time">20 Jun 2022 13:35:00 UTC</td>
          <td>Invited talk by Prof. Wei Shen</td>
        </tr>
        <tr>
          <td>2:15pm - 2:55pm</td>
          <td class="to-local-time">20 Jun 2022 14:15:00 UTC</td>
          <td>Invited talk by Prof. Gordon Wetzstein</td>
        </tr>
        <tr>
          <td>2:55pm - 3:00pm</td>
          <td class="to-local-time">20 Jun 2022 14:55:00 UTC</td>
          <td>Invited poster spotlight talk</td>
        </tr>
        <tr>
          <td>3:00pm - 4:00pm</td>
          <td class="to-local-time">20 Jun 2022 15:00:00 UTC</td>
          <td>Coffee break & poster presentation</td>
        </tr>
        <tr>
          <td>4:00pm - 5:10pm</td>
          <td class="to-local-time">20 Jun 2022 16:00:00 UTC</td>
          <td>Workshop paper presentation</td>
        </tr>
        <tr>
          <td>5:10pm - 5:50pm</td>
          <td class="to-local-time">20 Jun 2022 17:10:00 UTC</td>
          <td>Panel discussion</td>
        </tr>
        <tr>
          <td>5:50pm - 6:00pm</td>
          <td class="to-local-time">20 Jun 2022 17:50:00 UTC</td>
          <td>Award & closing remark</td>
        </tr>
        <!-- <tr>
          <td>8:15pm - 8:20pm</td>
          <td class="to-local-time">20 Jun 2021 20:15:00 UTC</td>
          <td>Award & closing remark</td>
        </tr> -->
      </tbody>
     </table>
     <span class="disclaimer">
     * This time is calculated to be in your computer's reported time zone.
     <br>
     For example, those in Los Angeles may see UTC-7,
     <br>
     while those in Berlin may see UTC+2.
     <br>
     <br>
     Please note that there may be differences to your actual time zone.</span>
  </div>
</div><br>

<!-- Speakers -->
<div class="row">
  <div class="col-xs-12"><a class="anchor" id="speakers"></a>
    <h2>Invited Speakers</h2>
    <br>

    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#speakers" href="#speakers-tab1" style="cursor:pointer;">
        <h3 style="margin:0;">Morning session </h3>
      </div>
      <div id="speakers-tab1" class="panel-collapse collapse" data-parent="#speakers">
        <div class="panel-body">

        <div class="row speaker">
            <div class="col-sm-3 speaker-pic">
              <a href="https://nin.nl/about-us/the-organisation/team/pieter-roelfsema//">
                <img class="people-pic" src="/2022/img/speakers/PR.jpg" />
              </a>
              <div class="people-name">
                <a href="https://nin.nl/about-us/the-organisation/team/pieter-roelfsema/">Pieter Roelfsema</a>
                <h6>Department Head, Netherlands Institute for Neuroscience </h6>
              </div>
            </div>
            <div class="col-sm-9">
              <h3>TBA</h3><br />
              <b>Abstract</b><p class="speaker-abstract">TBA.</p>
              <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/FUPpU_sT3LQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
              <!-- <div class="panel panel-default">
                <div class="panel-heading" data-toggle="collapse" href="#pr-bio" style="cursor:pointer;text-align:center">
                  <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
                </div>
                <div id="pr-bio" class="panel-collapse collapse"><div class="panel-body">
                  <p class="speaker-bio">
                    TBA
                  </p>
                </div></div> 
              </div>-->
            </div>
        </div>
          <br>

        <div class="row speaker">
            <div class="col-sm-3 speaker-pic">
              <a href="https://www.momen-nejad.org/">
                <img class="people-pic" src="/2022/img/speakers/IM.jpg" />
              </a>
              <div class="people-name">
                <a href="https://www.momen-nejad.org/">
                Ida Momennejad</a>
                <h6>Principal Researcher, Microsoft Research </h6>
              </div>
            </div>
            <div class="col-sm-9">
              <h3>TBA</h3><br />
              <b>Abstract</b><p class="speaker-abstract">TBA.</p>
              <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/FUPpU_sT3LQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
            </div>
        </div>
          <br>

        <div class="row speaker">
          <div class="col-sm-3 speaker-pic">
            <a href="https://www.jcrwhittington.com/">
              <img class="people-pic" src="/2022/img/speakers/JW.jpg" />
            </a>
            <div class="people-name">
              <a href="https://www.jcrwhittington.com/">
                James Whittington
              </a>
              <h6>Postdoc, University of Oxford </h6>
            </div>
          </div>
          <div class="col-sm-9">
            <h3>TBA</h3><br />
            <b>Abstract</b><p class="speaker-abstract">TBA.</p>
            <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/FUPpU_sT3LQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
          </div>
        </div>
          <br>
        
        <div class="row speaker">
          <div class="col-sm-3 speaker-pic">
            <a href="https://www.hennyadmoni.com/">
              <img class="people-pic" src="/2022/img/speakers/HA.png" />
            </a>
            <div class="people-name">
              <a href="https://www.hennyadmoni.com/">
                Henny Admoni
              </a>
              <h6>A. Nico Habermann Assistant Professor, Carnegie Mellon University </h6>
            </div>
          </div>
          <div class="col-sm-9">
            <h3>TBA</h3><br />
            <b>Abstract</b><p class="speaker-abstract">TBA.</p>
            <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/FUPpU_sT3LQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
          </div>
        </div>
          <br>  
          
        </div>
      </div>
    </div>

  <br>

<!-- hacky div to hide the accepted papers for now -->
<div style="display:none">
<!-- accepted papers  -->
  <div class="row">
    <div class="col-xs-12"><a class="anchor" id="accepted-papers"></a>
      <h2>Accepted Full Papers</h2>

    <div class="paper">
          <span class="title">Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation</span>
          <span class="authors">Jiawei Qin, Takuru Shimoyama, Yusuke Sugano</span>
          <span class="award">Best Paper Award</span>
          <div class="btn-group btn-group-xs" role="group">
            <button class="btn btn-success">GAZE 2022</button>
            <!--<button class="btn btn-poster-id">Poster #12Xa</button>-->
      <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
      <a class="btn btn-default" target="_blank" href="http://arxiv.org/abs/2201.07927"><i class="fas fa-archive"></i> arXiv</a>
        <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a>
            <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-code"></i> Code</a>-->
          </div>
      </div>

    <div class="paper">
          <span class="title">Self-Attention with Convolution and Deconvolution for Efficient Eye Gaze Estimation from a Full Face Image</span>
          <span class="authors">Jun O Oh, Hyung Jin Chang, Sang-Il Choi</span>
          <div class="btn-group btn-group-xs" role="group">
            <button class="btn btn-success">GAZE 2022</button>
            <!--<button class="btn btn-poster-id">Poster #12Xa</button>-->
      <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Oh_Self-Attention_With_Convolution_and_Deconvolution_for_Efficient_Eye_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
        <a class="btn btn-default" target="_blank" href="https://youtu.be/ANQ65NNNWNE"><i class="fas fa-video"></i> Video</a>
      <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>-->
      <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-archive"></i> arXiv</a>-->
            <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-code"></i> Code</a>-->
          </div>
      </div>

    <div class="paper">
          <span class="title">Unsupervised Multi-View Gaze Representation Learning</span>
          <span class="authors">John Gideon, Shan Su, Simon Stent</span>
          <span class="award">Best Poster Award</span>
          <div class="btn-group btn-group-xs" role="group">
            <button class="btn btn-success">GAZE 2022</button>
            <!--<button class="btn btn-poster-id">Poster #12Xa</button>-->
      <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gideon_Unsupervised_Multi-View_Gaze_Representation_Learning_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
        <a class="btn btn-default" target="_blank" href="https://youtu.be/W0OK1vVtiEk"><i class="fas fa-video"></i> Video</a>
      <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>-->
            <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-archive"></i> arXiv</a>-->
            <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-code"></i> Code</a>-->
          </div>
      </div>

    <div class="paper">
          <span class="title">ScanpathNet: A Recurrent Mixture Density Network for Scanpath Prediction</span>
          <span class="authors">Ryan Anthony J de Belen, Tomasz Bednarz, Arcot Sowmya</span>
          <span class="award">Best Paper Honourable Mention</span>
          <div class="btn-group btn-group-xs" role="group">
            <button class="btn btn-success">GAZE 2022</button>
            <!--<button class="btn btn-poster-id">Poster #12Xa</button>-->
      <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/de_Belen_ScanpathNet_A_Recurrent_Mixture_Density_Network_for_Scanpath_Prediction_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/de_Belen_ScanpathNet_A_Recurrent_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
        <a class="btn btn-default" target="_blank" href="https://youtu.be/8RXog3XkCl8"><i class="fas fa-video"></i> Video</a>
            <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-archive"></i> arXiv</a>-->
            <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-code"></i> Code</a>-->
          </div>
      </div>

    <div class="paper">
          <span class="title">One-Stage Object Referring with Gaze Estimation</span>
          <span class="authors">Jianhang Chen, Xu Zhang, Yue Wu, Shalini Ghosh, Pradeep Natarajan, Shih-Fu Chang, Jan Allebach</span>
          <div class="btn-group btn-group-xs" role="group">
            <button class="btn btn-success">GAZE 2022</button>
            <!--<button class="btn btn-poster-id">Poster #12Xa</button>-->
      <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_One-Stage_Object_Referring_With_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
        <a class="btn btn-default" target="_blank" href="https://youtu.be/SkjtCXX-aJY"><i class="fas fa-video"></i> Video</a>
      <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>-->
            <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-archive"></i> arXiv</a>-->
            <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-code"></i> Code</a>-->
          </div>
      </div>

    <div class="paper">
          <span class="title">Characterizing Target-absent Human Attention</span>
          <span class="authors">Yupei Chen, Zhibo Yang, Souradeep Chakraborty, Sounak Mondal, Seoyoung Ahn, Dimitris Samaras, Minh Hoai, Gregory Zelinsky</span>
          <div class="btn-group btn-group-xs" role="group">
            <button class="btn btn-success">GAZE 2022</button>
            <!--<button class="btn btn-poster-id">Poster #12Xa</button>-->
      <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_Characterizing_Target-Absent_Human_Attention_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Chen_Characterizing_Target-Absent_Human_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
        <a class="btn btn-default" target="_blank" href="https://youtu.be/SIVywYz2pNs"><i class="fas fa-video"></i> Video</a>
            <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-archive"></i> arXiv</a>-->
            <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-code"></i> Code</a>-->
          </div>
      </div>

    <br><br>

    <!-- <a class="anchor" id="invited-posters"></a>
      <h2>Invited Posters</h2>

    <div class="paper">
          <span class="title">Dynamic 3D Gaze from Afar: Deep Gaze Estimation from Temporal Eye-Head-Body Coordination</span>
          <span class="authors">Soma Nonaka, Shohei Nobuhara, Ko Nishino</span>
          <div class="btn-group btn-group-xs" role="group">
            <button class="btn btn-primary">CVPR 2022</button>
      
      <a class="btn btn-default" target="_blank" href="https://vision.ist.i.kyoto-u.ac.jp/pubs/SNonaka_CVPR22.pdf"><i class="fas fa-file-pdf"></i> PDF</a>
      <a class="btn btn-default" target="_blank" href="https://vision.ist.i.kyoto-u.ac.jp/pubs/SNonaka_CVPR22_supp.pdf"><i class="fas fa-file-pdf"></i> Supp.</a>
      <a class="btn btn-default" target="_blank" href=""><i class="fas fa-archive"></i> arXiv</a>
            <a class="btn btn-default" target="_blank" href="https://github.com/kyotovision-public/dynamic-3d-gaze-from-afar"><i class="fas fa-code"></i> Code</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/IEc8E4e4mXU"><i class="fas fa-video"></i> Video</a>
          </div>
      </div> 
      -->

    </div>
  </div>
  <br><br>
</div>

<!-- TODO update the PC -->
<div class="row" id="programcommittee">
  <div class="col-xs-12">
    <h2>Program Committee</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank"
      href=
      "https://ykotseruba.github.io/">
      Yulia Kotseruba</a><h6>
      York University</h6></div>
    <div class="people-name"><a target="_blank"
      href=
      "https://www.chengshuli.me/">
      Chengshu (Eric) Li</a><h6>
      Stanford University</h6></div>
    <div class="people-name"><a target="_blank"
      href=
      "https://sites.google.com/oakland.edu/human-centered-engineering">
      Hyungil Kim</a><h6>
      Oakland University</h6></div>
    <div class="people-name"><a target="_blank"
      href=
      "https://sushrutthorat.com/">
      Sushrut Thorat</a><h6>
      Radboud University</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank"
      href=
      "https://mklissa.github.io/">
      Martin Klissarov</a><h6>
      McGill University</h6></div>
    <div class="people-name"><a target="_blank"
        href=
        "https://david-abel.github.io/">
        David Abel</a><h6>
        Deepmind</h6>
    </div>
    <div class="people-name"><a target="_blank"
      href=
      "https://www.linkedin.com/in/david-nicholson-b27b69a4/">
      David Nicholson</a><h6>
      Embedded Intelligence</h6>
    </div>
    <div class="people-name"><a target="_blank"
      href=
      "http://wyblelab.com/">
      Brad Wyble</a><h6>
      Pennsylvania State University</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <div class="people-name"><a target="_blank"
      href=
      "https://prashnani.github.io/">
      Ekta Prashnani</a><h6>
      NVIDIA</h6>
    </div>  
  </div>
  </div>
</div>
<br>

<br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Organizers</h2>
  </div>
</div>

<br><br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://www.andrew.cmu.edu/user/abhijatb//">
      <img class="people-pic" src="{{ "img/organizers/AB.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.andrew.cmu.edu/user/abhijatb//">Abhijat Biswas</a>
      <h6>Carnegie Mellon University</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://kkhetarpal.github.io/">
      <img class="people-pic" src="{{ "img/organizers/KK.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://kkhetarpal.github.io/">Khimya Khetarpal</a>
      <h6>McGill University / MILA, Montreal</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="http://reuben-aronson.com/">
      <img class="people-pic" src="{{ "img/organizers/RA.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://reuben-aronson.com/">Reuben Aronson</a>
      <h6>Carnegie Mellon University / Tufts</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://asaran.github.io/">
      <img class="people-pic" src="{{ "img/organizers/AS.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://asaran.github.io/">Akanksha Saran</a>
      <h6>Microsoft Research</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ai.stanford.edu/~zharu/">
      <img class="people-pic" src="{{ "img/organizers/RZ.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ai.stanford.edu/~zharu/">Ruohan Zhang</a>
      <h6>Stanford University</h6>
    </div>
  </div>
  <div class="col-xs-1"></div>
</div>
<br>
<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="">
      <img class="people-pic" src="{{ "img/organizers/GL.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://gracewlindsay.com/">Grace Lindsay</a>
      <h6>University College London / New York University</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~sniekum/">
      <img class="people-pic" src="{{ "img/organizers/SN.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~sniekum/">Scott Niekum</a>
      <h6>University of Texas, Austin / University of Massachusets</h6>
    </div>
  </div>
</div><br>

<!-- <div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Website Chair</h2>
  </div>
</div> -->

<br><br>

<!-- <div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://hengfei-wang.github.io//github.io/">
      <img class="people-pic" src="{{ "img/people/hengfei.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hengfei Wang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <br><br>

</div>
<br> -->


<!-- <div class="row">
  <div class="col-xs-12"><a class="anchor" id="sponsors"></a>
    <h2>Workshop sponsored by:</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-4 sponsor">
    <a href="https://www.nvidia.com/"><img src="img/nvidia.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.tobii.com/"><img src="img/tobii.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.google.com/"><img src="img/google.png" /></a>
  </div>
</div> -->
